# Legacy's Mental Health CounselChat - RAG Implementation
# This notebook demonstrates the Retrieval-Augmented Generation workflow

import pandas as pd
import numpy as np
import torch
from transformers import RobertaForSequenceClassification, RobertaTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import openai
import time
from openai import OpenAIError
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# ========================================
# PART 1: Load Model, Tokenizer & Data
# ========================================

print("Loading model and tokenizer...")
checkpoint = torch.load(
    'models/rag_model_checkpoint.pth',
    map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),
    weights_only=False
)

tokenizer = checkpoint['tokenizer']
le = checkpoint['label_encoder']

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = RobertaForSequenceClassification.from_pretrained(
    'roberta-base',
    num_labels=len(le.classes_)
)
model.load_state_dict(checkpoint['model_state_dict'])
model.to(device)
model.eval()

print("âœ… Model, Tokenizer, and LabelEncoder loaded successfully!")

# ========================================
# PART 2: Topic Prediction Function
# ========================================

def predict_topic(text):
    """Predict mental health topic from input text."""
    inputs = tokenizer(
        text,
        return_tensors='pt',
        truncation=True,
        padding=True,
        max_length=512
    ).to(device)
    
    with torch.no_grad():
        outputs = model(**inputs)
        predicted_class = torch.argmax(outputs.logits, dim=1).item()
    
    return le.inverse_transform([predicted_class])[0]

# ========================================
# PART 3: Load Dataset & Setup TF-IDF
# ========================================

print("\nLoading dataset...")
df = pd.read_csv('data/counselchat-data.csv')

# Keep both questionText and answerText - CRITICAL for correct RAG!
df = df[['questionText', 'answerText', 'topic_group']].dropna()
df['questionText'] = df['questionText'].astype(str)
df['answerText'] = df['answerText'].astype(str)

print(f"Dataset shape: {df.shape}")

# Fit TF-IDF on QUESTIONS (we search questions, retrieve answers)
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
tfidf_matrix = vectorizer.fit_transform(df['questionText'])

print(f"âœ… TF-IDF Matrix Shape: {tfidf_matrix.shape}")

# ========================================
# PART 4: Similarity Search Function
# ========================================

def find_most_similar_answers(question, predicted_topic, vectorizer, tfidf_matrix, df, top_k=3):
    """
    Find top-K most similar questions and retrieve their answers.
    
    Key Logic:
    1. Filter dataset by predicted topic (if available)
    2. Transform user question to TF-IDF vector
    3. Calculate cosine similarity with all dataset questions
    4. Return top-K most similar question-answer pairs
    
    Args:
        question: User's input question
        predicted_topic: Topic predicted by classifier
        vectorizer: Fitted TF-IDF vectorizer
        tfidf_matrix: TF-IDF matrix of dataset questions
        df: DataFrame with questionText, answerText, topic_group
        top_k: Number of results to return
    
    Returns:
        List of dicts with 'question', 'answer', 'score'
    """
    # Filter by topic first
    if 'topic_group' in df.columns:
        topic_filtered_df = df[df['topic_group'] == predicted_topic].reset_index(drop=True)
        
        if len(topic_filtered_df) > 0:
            # Recalculate TF-IDF for filtered subset
            filtered_tfidf = vectorizer.transform(topic_filtered_df['questionText'])
            working_df = topic_filtered_df
            working_tfidf = filtered_tfidf
        else:
            # No matches in topic, use full dataset
            working_df = df
            working_tfidf = tfidf_matrix
    else:
        working_df = df
        working_tfidf = tfidf_matrix
    
    # Vectorize user question
    question_vector = vectorizer.transform([question])
    
    # Calculate similarities
    similarities = cosine_similarity(question_vector, working_tfidf).flatten()
    
    # Get top K indices
    top_indices = similarities.argsort()[-top_k:][::-1]
    
    # Build results
    results = []
    for idx in top_indices:
        if similarities[idx] > 0.1:  # Threshold
            results.append({
                'question': working_df.iloc[idx]['questionText'],
                'answer': working_df.iloc[idx]['answerText'],
                'score': float(similarities[idx])
            })
    
    return results if results else [{
        'question': 'No similar case found',
        'answer': 'Providing general guidance.',
        'score': 0.0
    }]

# ========================================
# PART 5: LLM Response Generation
# ========================================

# Setup OpenAI client
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    raise ValueError("OPENAI_API_KEY not found in environment variables!")

client = openai.OpenAI(api_key=api_key)

def generate_llm_response(topic, user_input, retrieved_answers, retries=3):
    """Generate advice using GPT-3.5 with retrieved context."""
    
    # Format context from retrieved answers
    context = "\n\n".join([
        f"Reference Case {i+1} (Similarity: {ans['score']:.2%}):\n"
        f"Patient Question: {ans['question']}\n"
        f"Counselor Response: {ans['answer']}"
        for i, ans in enumerate(retrieved_answers[:3])
    ])
    
    prompt = f"""
You are an empathetic mental health assistant.

**Patient's Concern:**
"{user_input}"

**Topic:** {topic}

**Similar Cases from Professional Counselors:**
{context}

**Instructions:**
- Directly address: "{user_input}"
- If references are too specific, generalize appropriately
- Be warm, empathetic, and actionable

**Provide a structured response:**
1. **Understanding the Challenge:** Validate their feelings (2-3 sentences)
2. **Practical Advice:** 2-3 clear, actionable steps
3. **Emotional Support:** Coping strategies
4. **Encouragement:** Positive closing message

Keep response under 400 tokens.
"""
    
    for attempt in range(retries):
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are an empathetic mental health assistant."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        
        except openai.RateLimitError:
            wait_time = 2 ** attempt
            print(f"âš ï¸ Rate limit exceeded. Retrying in {wait_time} seconds...")
            time.sleep(wait_time)
        
        except OpenAIError as e:
            print(f"âŒ OpenAI API Error: {e}")
            break
    
    return "âš ï¸ Unable to fetch advice due to API issues."

# ========================================
# PART 6: Test the Complete Pipeline
# ========================================

print("\n" + "="*60)
print("TESTING COMPLETE RAG PIPELINE")
print("="*60)

test_cases = [
    "I feel anxious all the time, what should I do?",
    "My partner and I are having communication issues",
    "I want to quit smoking but can't seem to stop",
    "I'm struggling to come to terms with my sexual orientation",
    "I'm experiencing burnout from my high-pressure job"
]

for i, test_input in enumerate(test_cases, 1):
    print(f"\n{'='*60}")
    print(f"TEST CASE {i}")
    print(f"{'='*60}")
    print(f"ðŸ“ User Input: {test_input}")
    
    # Step 1: Predict Topic
    predicted_topic = predict_topic(test_input)
    print(f"ðŸ”® Predicted Topic: {predicted_topic}")
    
    # Step 2: Retrieve Similar Cases
    retrieved = find_most_similar_answers(
        test_input,
        predicted_topic,
        vectorizer,
        tfidf_matrix,
        df,
        top_k=3
    )
    
    print(f"\nðŸ“š Retrieved {len(retrieved)} Similar Cases:")
    for j, case in enumerate(retrieved, 1):
        print(f"\nCase {j} (Score: {case['score']:.3f}):")
        print(f"  Q: {case['question'][:100]}...")
        print(f"  A: {case['answer'][:100]}...")
    
    # Step 3: Generate LLM Response
    print("\nðŸ¤– Generating LLM Response...")
    llm_advice = generate_llm_response(predicted_topic, test_input, retrieved)
    
    print(f"\nðŸ’¡ LLM Advice:\n{llm_advice}")
    print(f"\n{'='*60}\n")
    
    # Add delay to avoid rate limits
    time.sleep(2)

print("\nâœ… All test cases completed!")

# ========================================
# PART 7: Interactive Testing
# ========================================

def interactive_test():
    """Run an interactive test session."""
    print("\n" + "="*60)
    print("INTERACTIVE MODE - Enter 'quit' to exit")
    print("="*60)
    
    while True:
        user_input = input("\nðŸ’¬ Enter patient's concern: ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'q']:
            print("ðŸ‘‹ Exiting interactive mode.")
            break
        
        if not user_input:
            print("âš ï¸ Please enter a valid concern.")
            continue
        
        # Run pipeline
        predicted_topic = predict_topic(user_input)
        print(f"\nðŸ”® Predicted Topic: {predicted_topic}")
        
        retrieved = find_most_similar_answers(
            user_input,
            predicted_topic,
            vectorizer,
            tfidf_matrix,
            df,
            top_k=3
        )
        
        print(f"\nðŸ“š Found {len(retrieved)} similar cases (showing top match):")
        if retrieved[0]['score'] > 0:
            print(f"  Similarity: {retrieved[0]['score']:.2%}")
            print(f"  Question: {retrieved[0]['question'][:150]}...")
        
        llm_advice = generate_llm_response(predicted_topic, user_input, retrieved)
        print(f"\nðŸ’¡ Generated Advice:\n{llm_advice}")

# Uncomment to run interactive mode:
# interactive_test()

# ========================================
# PART 2: Topic Prediction Function
# ========================================

def predict_topic(text):
    """Predict mental health topic from input text."""
    inputs = tokenizer(
        text,
        return_tensors='pt',
        truncation=True,
        padding=True,
        max_length=512
    ).to(device)
    
    with torch.no_grad():
        outputs = model(**inputs)
        predicted_class = torch.argmax(outputs.logits, dim=1).item()
    
    return le.inverse_transform([predicted_class])[0]

# ========================================
# PART 3: Load Dataset & Setup TF-IDF
# ========================================

print("\nLoading dataset...")
df = pd.read_csv('data/counselchat-data.csv')

# Keep both questionText and answerText - CRITICAL for correct RAG!
df = df[['questionText', 'answerText', 'topic_group']].dropna()
df['questionText'] = df['questionText'].astype(str)
df['answerText'] = df['answerText'].astype(str)

print(f"Dataset shape: {df.shape}")

# Fit TF-IDF on QUESTIONS (we search questions, retrieve answers)
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
tfidf_matrix = vectorizer.fit_transform(df['questionText'])

print(f"âœ… TF-IDF Matrix Shape: {tfidf_matrix.shape}")

# ========================================
# PART 4: Similarity Search Function
# ========================================

def find_most_similar_answers(question, predicted_topic, vectorizer, tfidf_matrix, df, top_k=3):
    """
    Find top-K most similar questions and retrieve their answers.
    
    Key Logic:
    1. Filter dataset by predicted topic (if available)
    2. Transform user question to TF-IDF vector
    3. Calculate cosine similarity with all dataset questions
    4. Return top-K most similar question-answer pairs
    
    Args:
        question: User's input question
        predicted_topic: Topic predicted by classifier
        vectorizer: Fitted TF-IDF vectorizer
        tfidf_matrix: TF-IDF matrix of dataset questions
        df: DataFrame with questionText, answerText, topic_group
        top_k: Number of results to return
    
    Returns:
        List of dicts with 'question', 'answer', 'score'
    """
    # Filter by topic first
    if 'topic_group' in df.columns:
        topic_filtered_df = df[df['topic_group'] == predicted_topic].reset_index(drop=True)
        
        if len(topic_filtered_df) > 0:
            # Recalculate TF-IDF for filtered subset
            filtered_tfidf = vectorizer.transform(topic_filtered_df['questionText'])
            working_df = topic_filtered_df
            working_tfidf = filtered_tfidf
        else:
            # No matches in topic, use full dataset
            working_df = df
            working_tfidf = tfidf_matrix
    else:
        working_df = df
        working_tfidf = tfidf_matrix
    
    # Vectorize user question
    question_vector = vectorizer.transform([question])
    
    # Calculate similarities
    similarities = cosine_similarity(question_vector, working_tfidf).flatten()
    
    # Get top K indices
    top_indices = similarities.argsort()[-top_k:][::-1]
    
    # Build results
    results = []
    for idx in top_indices:
        if similarities[idx] > 0.1:  # Threshold
            results.append({
                'question': working_df.iloc[idx]['questionText'],
                'answer': working_df.iloc[idx]['answerText'],
                'score': float(similarities[idx])
            })
    
    return results if results else [{
        'question': 'No similar case found',
        'answer': 'Providing general guidance.',
        'score': 0.0
    }]

# ========================================
# PART 5: LLM Response Generation
# ========================================

# Setup OpenAI client
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    raise ValueError("OPENAI_API_KEY not found in environment variables!")

client = openai.OpenAI(api_key=api_key)

def generate_llm_response(topic, user_input, retrieved_answers, retries=3):
    """Generate advice using GPT-3.5 with retrieved context."""
    
    # Format context from retrieved answers
    context = "\n\n".join([
        f"Reference Case {i+1} (Similarity: {ans['score']:.2%}):\n"
        f"Patient Question: {ans['question']}\n"
        f"Counselor Response: {ans['answer']}"
        for i, ans in enumerate(retrieved_answers[:3])
    ])
    
    prompt = f"""
You are an empathetic mental health assistant.

**Patient's Concern:**
"{user_input}"

**Topic:** {topic}

**Similar Cases from Professional Counselors:**
{context}

**Instructions:**
- Directly address: "{user_input}"
- If references are too specific, generalize appropriately
- Be warm, empathetic, and actionable

**Provide a structured response:**
1. **Understanding the Challenge:** Validate their feelings (2-3 sentences)
2. **Practical Advice:** 2-3 clear, actionable steps
3. **Emotional Support:** Coping strategies
4. **Encouragement:** Positive closing message

Keep response under 400 tokens.
"""
    
    for attempt in range(retries):
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are an empathetic mental health assistant."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        
        except openai.RateLimitError:
            wait_time = 2 ** attempt
            print(f"âš ï¸ Rate limit exceeded. Retrying in {wait_time} seconds...")
            time.sleep(wait_time)
        
        except OpenAIError as e:
            print(f"âŒ OpenAI API Error: {e}")
            break
    
    return "âš ï¸ Unable to fetch advice due to API issues."

# ========================================
# PART 6: Test the Complete Pipeline
# ========================================

print("\n" + "="*60)
print("TESTING COMPLETE RAG PIPELINE")
print("="*60)

test_cases = [
    "I feel anxious all the time, what should I do?",
    "My partner and I are having communication issues",
    "I want to quit smoking but can't seem to stop",
    "I'm struggling to come to terms with my sexual orientation",
    "I'm experiencing burnout from my high-pressure job"
]

for i, test_input in enumerate(test_cases, 1):
    print(f"\n{'='*60}")
    print(f"TEST CASE {i}")
    print(f"{'='*60}")
    print(f"ðŸ“ User Input: {test_input}")
    
    # Step 1: Predict Topic
    predicted_topic = predict_topic(test_input)
    print(f"ðŸ”® Predicted Topic: {predicted_topic}")
    
    # Step 2: Retrieve Similar Cases
    retrieved = find_most_similar_answers(
        test_input,
        predicted_topic,
        vectorizer,
        tfidf_matrix,
        df,
        top_k=3
    )
    
    print(f"\nðŸ“š Retrieved {len(retrieved)} Similar Cases:")
    for j, case in enumerate(retrieved, 1):
        print(f"\nCase {j} (Score: {case['score']:.3f}):")
        print(f"  Q: {case['question'][:100]}...")
        print(f"  A: {case['answer'][:100]}...")
    
    # Step 3: Generate LLM Response
    print("\nðŸ¤– Generating LLM Response...")
    llm_advice = generate_llm_response(predicted_topic, test_input, retrieved)
    
    print(f"\nðŸ’¡ LLM Advice:\n{llm_advice}")
    print(f"\n{'='*60}\n")
    
    # Add delay to avoid rate limits
    time.sleep(2)

print("\nâœ… All test cases completed!")

# ========================================
# PART 7: Interactive Testing
# ========================================

def interactive_test():
    """Run an interactive test session."""
    print("\n" + "="*60)
    print("INTERACTIVE MODE - Enter 'quit' to exit")
    print("="*60)
    
    while True:
        user_input = input("\nðŸ’¬ Enter patient's concern: ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'q']:
            print("ðŸ‘‹ Exiting interactive mode.")
            break
        
        if not user_input:
            print("âš ï¸ Please enter a valid concern.")
            continue
        
        # Run pipeline
        predicted_topic = predict_topic(user_input)
        print(f"\nðŸ”® Predicted Topic: {predicted_topic}")
        
        retrieved = find_most_similar_answers(
            user_input,
            predicted_topic,
            vectorizer,
            tfidf_matrix,
            df,
            top_k=3
        )
        
        print(f"\nðŸ“š Found {len(retrieved)} similar cases (showing top match):")
        if retrieved[0]['score'] > 0:
            print(f"  Similarity: {retrieved[0]['score']:.2%}")
            print(f"  Question: {retrieved[0]['question'][:150]}...")
        
        llm_advice = generate_llm_response(predicted_topic, user_input, retrieved)
        print(f"\nðŸ’¡ Generated Advice:\n{llm_advice}")

# Uncomment to run interactive mode:
# interactive_test()